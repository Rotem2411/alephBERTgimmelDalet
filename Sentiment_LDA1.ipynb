{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWkoCc2WyVEJsx+KvmKEws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rotem2411/alephBERTgimmelDalet/blob/main/Sentiment_LDA1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YrscUhc4RI_T",
        "outputId": "ce326f31-0601-4df1-9a4a-86020e2ccde4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_reader = csv.reader(csvfile)\n",
        "        for row in csv_reader:\n",
        "            data.append(row)\n",
        "    return data"
      ],
      "metadata": {
        "id": "K8ZPx8juRlVP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'sentiments.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "UC7XwJZAmiY4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a peek of the data"
      ],
      "metadata": {
        "id": "D3BERjvkmlbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(df)\n",
        "print(\"Total number of samples: \", num_samples)\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIWzcPBPRtoP",
        "outputId": "327029a1-49c8-45f5-e448-2b5dbe80bc4b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples:  75151\n",
            "   id                                               text tag category class  \\\n",
            "0   1                    האריות של הצל חזק פה בתגובות...   ש  ECONOMY    b1   \n",
            "1   2  זמרת תעשייה רק מפרסומות מפורסמת\\n  אבל אין קהל...   ש  ECONOMY    b1   \n",
            "2   4  את לא יורקת לבאר שממנה שתית. יפה יפה, אבל חסרת...   ש  ECONOMY    b1   \n",
            "3   5  שמעון\\n  משתתף בפרסומת למילקי במקום לעורר מודע...   ש  ECONOMY    b1   \n",
            "4   6  הצחקתם אותי\\n  מה כל כך אמייזינג בחברה עם אפס ...   ש  ECONOMY    b1   \n",
            "5   7  איסו חברת הייטק מצליחה צריכה פרסומות שיבואו לע...   ש  ECONOMY    b1   \n",
            "6   8           לא מכיר אותה ולא שמעתי אפילו שיר אחד שלה   ש  ECONOMY    b1   \n",
            "7   9  וואללה לא יודע מה מתלהבים מנגה ארז. זה בכלל לא...   ש  ECONOMY    b1   \n",
            "8  10  אהובה שגיא\\n  נגה גדולה.זמרת מעולה.פרסומת מדלי...   ח  ECONOMY    b1   \n",
            "9  11  הפרסומת של אמדוקס נראית מאוד קודרת ואפורה, כמו...   ש  ECONOMY    b1   \n",
            "\n",
            "   total_tags  selected_tag  polarity  \n",
            "0           2             2       1.0  \n",
            "1           2             2       1.0  \n",
            "2           2             2       1.0  \n",
            "3           2             2       1.0  \n",
            "4           2             2       1.0  \n",
            "5           2             2       1.0  \n",
            "6           2             2       1.0  \n",
            "7           2             2       1.0  \n",
            "8           2             2       1.0  \n",
            "9           2             2       1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment distribution\n",
        "Sentiment_distribution = {}\n",
        "for index, row in df.iterrows():\n",
        "    Sentiment_label = row['tag']\n",
        "    if Sentiment_label not in Sentiment_distribution:\n",
        "        Sentiment_distribution[Sentiment_label] = 1\n",
        "    else:\n",
        "        Sentiment_distribution[Sentiment_label] += 1\n",
        "print(\"Sentiment Distribution:\")\n",
        "for Sentiment_label, count in Sentiment_distribution.items():\n",
        "    print(f\"{Sentiment_label}: {count} tags\")"
      ],
      "metadata": {
        "id": "M9ETQrBhqPIB",
        "outputId": "e8dc8a17-ac89-473e-8f9e-4894c58e139f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Distribution:\n",
            "ש: 60841 tags\n",
            "ח: 6982 tags\n",
            "נ: 7328 tags\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Category distribution\n",
        "Category_distribution = {}\n",
        "for index, row in df.iterrows():\n",
        "  Category_label = row['category']\n",
        "  if Category_label not in Category_distribution:\n",
        "    Category_distribution[Category_label] = 1\n",
        "  else:\n",
        "    Category_distribution[Category_label] += 1\n",
        "print(\"Category Distribution:\")\n",
        "for Category_label, count in Category_distribution.items():\n",
        "  print(f\"{Category_label}: {count} category\")"
      ],
      "metadata": {
        "id": "H8SuWsXGoPWk",
        "outputId": "66f7bddc-b574-4d02-edde-2670cb2041a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category Distribution:\n",
            "ECONOMY: 40923 category\n",
            "NEWS: 24905 category\n",
            "sport: 9323 category\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = df['text']\n",
        "labels = df['category']"
      ],
      "metadata": {
        "id": "5ItCQOMIxUcH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Pre-processing**\n",
        "\n",
        "We will perform the following steps:\n",
        "\n",
        "\n",
        "*   **Tokenization**: Split the text into sentences and the sentences into words\n",
        "*   Remove **punctuation**.\n",
        "*   All **stopwords** are removed.\n",
        "*   Words are **lemmatized** — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
        "*   Words are **stemmed** — words are reduced to their root form."
      ],
      "metadata": {
        "id": "t4QOQyyIm7zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanwords(text):\n",
        "    wn = nltk.WordNetLemmatizer()\n",
        "    stopword = nltk.corpus.stopwords.words('hebrew')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    no_stopwords = [word for word in tokens if word not in stopword]\n",
        "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
        "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
        "    return lemm_text"
      ],
      "metadata": {
        "id": "mC5JkMWFCX5C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before cleaning the text:"
      ],
      "metadata": {
        "id": "fnLhuWNLurqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words[0]"
      ],
      "metadata": {
        "id": "WqhOw20spp8U",
        "outputId": "537cf3e1-72ee-43ff-edb3-49e2cea6a764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'האריות של הצל חזק פה בתגובות...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cleaning the text:"
      ],
      "metadata": {
        "id": "6GP8wlPGunff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleanwords(words[0])"
      ],
      "metadata": {
        "id": "-qJI-cjtptgJ",
        "outputId": "eeb84d8a-d712-459e-ada3-f7973e2c4d5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['האריות', 'הצל', 'חזק', 'בתגובות']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_words = {}\n",
        "for i in words.index:\n",
        "  clean_words[i] = cleanwords(words[i])"
      ],
      "metadata": {
        "id": "97YfgQRisZqh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the data to train(80%), valid(10%) and test(10%)"
      ],
      "metadata": {
        "id": "XNq4F2fuiYkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(clean_words,labels,test_size = 0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size = 0.5, random_state=42)\n",
        "\n",
        "print(\"Data distribution:\\n- Train: {} \\n- Validation: {} \\n- Test: {}\".format(len(y_train),len(y_val),len(y_test)))"
      ],
      "metadata": {
        "id": "DxVoreGsxg4j",
        "outputId": "46935c53-c981-4614-e1f2-632240687bba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data distribution:\n",
            "- Train: 60120 \n",
            "- Validation: 7515 \n",
            "- Test: 7516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "WzVKc6VBi5Q8",
        "outputId": "5d37ef5d-f06b-4b69-a8e9-90aac8cbca7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['לפי', 'שהמורה', 'לגאומטריה', 'לימדה', 'צודקים', 'הפעם']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [\" \".join(x) for x in X_train]"
      ],
      "metadata": {
        "id": "ckKhIfIi991i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(data, tfidf_vect_fit):\n",
        "    X_tfidf = tfidf_vect_fit.transform(data)\n",
        "    words = tfidf_vect_fit.get_feature_names_out()\n",
        "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
        "    X_tfidf_df.columns = words\n",
        "    return(X_tfidf_df)"
      ],
      "metadata": {
        "id": "kk-Apm3K--Gv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA - Latent Dirichlet allocation"
      ],
      "metadata": {
        "id": "0WfW-hsXqBMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vect_fit=tfidf_vect.fit(X_train)\n",
        "X_train = vectorize(X_train, tfidf_vect_fit)\n",
        "tfidf_matrix = tfidf_vect.fit_transform(X_train)\n",
        "num_topics = 3\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=100)\n",
        "lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
        "# Print the top words for each topic\n",
        "feature_names = tfidf_vect.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    top_words = [feature_names[i] for i in topic.argsort()[:-31:-1]]\n",
        "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
      ],
      "metadata": {
        "id": "Eu5nRy44gi70",
        "outputId": "b50fb192-685a-4144-f0d1-f2bd3ac4292e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: תטפח, שבשום, מייננת, איימים, במיגזר, שמסננים, דורכת, עירם, יומה, המוסדיים, התגשם, משדה, לתקצב, הכפרה, מהסדר, מוותרת, למחירם, שרשם, מנקי, המצומצם, מיזוגניים, תאתרו, לסגוד, בשגיאות, לתפוזים, מבוגרות, והאחרונה, ריססו, מצבור, המזרן\n",
            "Topic 2: פסיכיאטרים, שהתחלפה, חנזיר, ממגע, והשגוי, נדל, מסודרות, התספורות, ייגבו, היזמין, להחלים, טכנים, סתוק, חבצע, אוטורו, שמתקדמים, ומסרבלים, החקירות, מחסן, הענף, למוקד, הקניית, ומסרים, מענים, מהמככבים, מהעבודה, בכללם, לעמדתם, לקולות, לייצמן\n",
            "Topic 3: ניסנובסטוק, פליישר, למשפטים, כככככככןןןןןןןןןןןןןןן, הלהג, בילדים, כהודאה, ביטוחעם, מתחרטת, סרת, האומנם, הזינוק, ממרוקאי, למפזר, שמרנות, מתגעגעים, כבנט, וגולת, שיקרתם, כיסי, קלט, תפילטר, בטובת, רחבה, מרפדיה, זידאן, הריסות, הריקה, כשהציעו, ברור\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# Generate the word cloud.\n",
        "text = str(feature_names)\n",
        "text = cleanwords(text)\n",
        "wordcloud = WordCloud().generate(text)\n",
        "\n",
        "# Display the word cloud.\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Top words for {topic_idx}\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6yNW4q6F7mtT",
        "outputId": "ad195711-871b-40a5-b58d-f6dec20a03cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-85d4f0c96424>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Display the word cloud.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;31m# remove 's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = str(feature_names)\n",
        "text"
      ],
      "metadata": {
        "id": "NJlZDs97_672"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = cleanwords(text)\n",
        "text"
      ],
      "metadata": {
        "id": "9PMsB-vBFA0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}