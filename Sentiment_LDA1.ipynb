{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0DNUgMr7r0l6SvsI0cyyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rotem2411/alephBERTgimmelDalet/blob/main/Sentiment_LDA1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YrscUhc4RI_T"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_reader = csv.reader(csvfile)\n",
        "        for row in csv_reader:\n",
        "            data.append(row)\n",
        "    return data"
      ],
      "metadata": {
        "id": "K8ZPx8juRlVP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    file_path = 'sentiments.csv'\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(df.head(10))\n",
        "\n",
        "    # Total number of samples\n",
        "    num_samples = len(df)\n",
        "    print(\"Total number of samples:\", num_samples)\n",
        "\n",
        "    # Sentiment distribution\n",
        "    Sentiment_distribution = {}\n",
        "    for index, row in df.iterrows():\n",
        "        Sentiment_label = row['tag']\n",
        "        if Sentiment_label not in Sentiment_distribution:\n",
        "            Sentiment_distribution[Sentiment_label] = 1\n",
        "        else:\n",
        "            Sentiment_distribution[Sentiment_label] += 1\n",
        "    print(\"Sentiment Distribution:\")\n",
        "    for Sentiment_label, count in Sentiment_distribution.items():\n",
        "        print(f\"{Sentiment_label}: {count} tags\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIWzcPBPRtoP",
        "outputId": "8f5c58f6-1156-436e-aeb9-51383d4d708e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                               text tag category class  \\\n",
            "0   1                    האריות של הצל חזק פה בתגובות...   ש  ECONOMY    b1   \n",
            "1   2  זמרת תעשייה רק מפרסומות מפורסמת\\n  אבל אין קהל...   ש  ECONOMY    b1   \n",
            "2   4  את לא יורקת לבאר שממנה שתית. יפה יפה, אבל חסרת...   ש  ECONOMY    b1   \n",
            "3   5  שמעון\\n  משתתף בפרסומת למילקי במקום לעורר מודע...   ש  ECONOMY    b1   \n",
            "4   6  הצחקתם אותי\\n  מה כל כך אמייזינג בחברה עם אפס ...   ש  ECONOMY    b1   \n",
            "5   7  איסו חברת הייטק מצליחה צריכה פרסומות שיבואו לע...   ש  ECONOMY    b1   \n",
            "6   8           לא מכיר אותה ולא שמעתי אפילו שיר אחד שלה   ש  ECONOMY    b1   \n",
            "7   9  וואללה לא יודע מה מתלהבים מנגה ארז. זה בכלל לא...   ש  ECONOMY    b1   \n",
            "8  10  אהובה שגיא\\n  נגה גדולה.זמרת מעולה.פרסומת מדלי...   ח  ECONOMY    b1   \n",
            "9  11  הפרסומת של אמדוקס נראית מאוד קודרת ואפורה, כמו...   ש  ECONOMY    b1   \n",
            "\n",
            "   total_tags  selected_tag  polarity  \n",
            "0           2             2       1.0  \n",
            "1           2             2       1.0  \n",
            "2           2             2       1.0  \n",
            "3           2             2       1.0  \n",
            "4           2             2       1.0  \n",
            "5           2             2       1.0  \n",
            "6           2             2       1.0  \n",
            "7           2             2       1.0  \n",
            "8           2             2       1.0  \n",
            "9           2             2       1.0  \n",
            "Total number of samples: 75151\n",
            "Sentiment Distribution:\n",
            "ש: 60841 tags\n",
            "ח: 6982 tags\n",
            "נ: 7328 tags\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = df['text']\n",
        "labels = df['tag']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size = 0.2, random_state=42)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size = 0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size = 0.5, random_state=42)\n",
        "\n",
        "print(\"Data distribution:\\n- Train: {} \\n- Validation: {} \\n- Test: {}\".format(len(y_train),len(y_val),len(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFGI51fVkaFf",
        "outputId": "b673ccdb-2529-4012-e441-d743543c4f4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data distribution:\n",
            "- Train: 60120 \n",
            "- Validation: 7515 \n",
            "- Test: 7516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanwords(text):\n",
        "    wn = nltk.WordNetLemmatizer()\n",
        "    stopword = nltk.corpus.stopwords.words('hebrew')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    lower = [word.lower() for word in tokens]\n",
        "    no_stopwords = [word for word in lower if word not in stopword]\n",
        "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
        "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
        "    clean_text = lemm_text\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "mC5JkMWFCX5C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "tfidf_vect = TfidfVectorizer(analyzer=cleanwords)\n",
        "tfidf_vect_fit=tfidf_vect.fit(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Na-qs0OdU4A",
        "outputId": "e62fbb46-f16a-473f-9f7e-7f3c16e31a38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(data, tfidf_vect_fit):\n",
        "    X_tfidf = tfidf_vect_fit.transform(data)\n",
        "    words = tfidf_vect_fit.get_feature_names_out()\n",
        "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
        "    X_tfidf_df.columns = words\n",
        "    return(X_tfidf_df)"
      ],
      "metadata": {
        "id": "kk-Apm3K--Gv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectorize(X_train, tfidf_vect_fit)"
      ],
      "metadata": {
        "id": "weLzkKSSGXsn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA - Latent Dirichlet allocation"
      ],
      "metadata": {
        "id": "0WfW-hsXqBMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = tfidf_vect.fit_transform(X_train)\n",
        "num_topics = 3\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=100)\n",
        "lda_matrix = lda_model.fit_transform(tfidf_matrix)"
      ],
      "metadata": {
        "id": "OwDiuPpcTXt9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top words for each topic\n",
        "feature_names = tfidf_vect.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
        "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0byaz8BPv3q",
        "outputId": "4f0208d6-1212-4fbb-e212-6e6ec4ea9aef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: עויין, שבחירות, ואופא, שמחקו, עורכת, קצורי, ללמידה, מאפילה, קוק, ומסתיים\n",
            "Topic 2: חינמית, ובמשבר, יורדות, צברתי, החזיק, הטכנולוגייה, מנובלים, התייעלו, מקדשת, כלום\n",
            "Topic 3: מיעוטם, במעטפות, משתמשיים, לאחד, שמספר, שמיובאים, מתחסנות, בטיחותית, הסמטלנים, והחברים\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# Generate the word cloud.\n",
        "text = str(feature_names)\n",
        "text = cleanwords(text)\n",
        "wordcloud = WordCloud().generate(text)\n",
        "\n",
        "# Display the word cloud.\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Top words for {topic_idx}\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6yNW4q6F7mtT",
        "outputId": "e0e4e07c-8d2a-4ff6-f0de-c3cf3adb5222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-85d4f0c96424>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Display the word cloud.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;31m# remove 's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = str(feature_names)\n",
        "text"
      ],
      "metadata": {
        "id": "NJlZDs97_672",
        "outputId": "e49daa2e-8979-4a48-d2e7-ea1df6acd9a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['a' 'about' 'accidentally' ... 'ײסודה' 'בּטוח' 'בּמקצועם']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = cleanwords(text)\n",
        "text"
      ],
      "metadata": {
        "id": "9PMsB-vBFA0B",
        "outputId": "b96cd761-7dd3-49b7-e488-f7d6b1d0ba04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}