{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe4sDUfF7hAWC5p4f4jzCn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rotem2411/alephBERTgimmelDalet/blob/main/word_embedding1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S9GF-Ohg-XF6",
        "outputId": "ec36021b-21cb-4941-f49d-f7be738fe1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'sentiments.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "ChRe4f3D-aBZ",
        "outputId": "808c173a-3d18-4954-dfc4-a5eedb19677b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sentiments.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1b9f440c3616>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentiments.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sentiments.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert words to embeddings without text cleaning, using Word2Vec model"
      ],
      "metadata": {
        "id": "D_glZXOBj2BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = df['text'].tolist()\n",
        "tokenized_text = [nltk.word_tokenize(text) for text in docs]\n",
        "model = Word2Vec(tokenized_text, min_count=1, vector_size=20, window=5)"
      ],
      "metadata": {
        "id": "8eZVCqn0-q82"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of most similar words to 'נתניהו'"
      ],
      "metadata": {
        "id": "7KMewKzzf7K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(model.wv.most_similar('נתניהו', topn=10))"
      ],
      "metadata": {
        "id": "a2W7da0Sd6R2",
        "outputId": "db781999-6b0b-4e13-d2e8-f70a7fe1a249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('הליכוד', 0.9680353999137878),\n",
              " ('פוטין', 0.9610236287117004),\n",
              " ('לפיד', 0.9584319591522217),\n",
              " ('הפרצוף', 0.9553976058959961),\n",
              " ('בנט', 0.9455683827400208),\n",
              " ('טראמפ', 0.9421078562736511),\n",
              " ('ביב', 0.9407416582107544),\n",
              " ('המבחן', 0.9383149147033691),\n",
              " ('יאיר', 0.9372230768203735),\n",
              " ('ליברמן', 0.9363155961036682)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document Vectors from Word Embedding"
      ],
      "metadata": {
        "id": "d8iUpqHthBM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(list_of_docs, model):\n",
        "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
        "\n",
        "    Args:\n",
        "        list_of_docs: List of documents\n",
        "        model: Gensim's Word Embedding\n",
        "\n",
        "    Returns:\n",
        "        List of document vectors\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    for tokens in list_of_docs:\n",
        "        zero_vector = np.zeros(model.vector_size)\n",
        "        vectors = []\n",
        "        for token in tokens:\n",
        "            if token in model.wv:\n",
        "                try:\n",
        "                    vectors.append(model.wv[token])\n",
        "                except KeyError:\n",
        "                    continue\n",
        "        if vectors:\n",
        "            vectors = np.asarray(vectors)\n",
        "            avg_vec = vectors.mean(axis=0)\n",
        "            features.append(avg_vec)\n",
        "        else:\n",
        "            features.append(zero_vector)\n",
        "    return features\n",
        "\n",
        "vectorized_text = vectorize(tokenized_text, model=model)\n",
        "len(vectorized_text), len(vectorized_text[0])"
      ],
      "metadata": {
        "id": "XUOkMit8gpYc",
        "outputId": "e3b4a5af-472b-4f75-dff2-bb8e173543c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75151, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply K-means clustering"
      ],
      "metadata": {
        "id": "akhIIa0MjmCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mbkmeans_clusters(\n",
        "\tX,\n",
        "    k,\n",
        "    mb,\n",
        "    print_silhouette_values,\n",
        "):\n",
        "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
        "\n",
        "    Args:\n",
        "        X: Matrix of features.\n",
        "        k: Number of clusters.\n",
        "        mb: Size of mini-batches.\n",
        "        print_silhouette_values: Print silhouette values per cluster.\n",
        "\n",
        "    Returns:\n",
        "        Trained clustering model and labels based on X.\n",
        "    \"\"\"\n",
        "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
        "    print(f\"For n_clusters = {k}\")\n",
        "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
        "    print(f\"Inertia:{km.inertia_}\")\n",
        "\n",
        "    if print_silhouette_values:\n",
        "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
        "        print(f\"Silhouette values:\")\n",
        "        silhouette_values = []\n",
        "        for i in range(k):\n",
        "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
        "            silhouette_values.append(\n",
        "                (\n",
        "                    i,\n",
        "                    cluster_silhouette_values.shape[0],\n",
        "                    cluster_silhouette_values.mean(),\n",
        "                    cluster_silhouette_values.min(),\n",
        "                    cluster_silhouette_values.max(),\n",
        "                )\n",
        "            )\n",
        "        silhouette_values = sorted(\n",
        "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
        "        )\n",
        "        for s in silhouette_values:\n",
        "            print(\n",
        "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
        "            )\n",
        "    return km, km.labels_"
      ],
      "metadata": {
        "id": "2GoqZwLJgiA9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering, cluster_labels = mbkmeans_clusters(\n",
        "\tX=vectorized_text,\n",
        "    k=50,\n",
        "    mb=500,\n",
        "    print_silhouette_values=True,\n",
        ")\n",
        "df_clusters = pd.DataFrame({\n",
        "    \"text\": docs,\n",
        "    \"tokens\": [\" \".join(text) for text in tokenized_text],\n",
        "    \"cluster\": cluster_labels\n",
        "})"
      ],
      "metadata": {
        "id": "psDpi2L1hHKD",
        "outputId": "ed879347-0cea-4009-b5b0-b107d54ef240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For n_clusters = 50\n",
            "Silhouette coefficient: 0.04\n",
            "Inertia:155906.8827806821\n",
            "Silhouette values:\n",
            "    Cluster 37: Size:1312 | Avg:0.33 | Min:0.02 | Max: 0.53\n",
            "    Cluster 19: Size:555 | Avg:0.14 | Min:-0.11 | Max: 0.38\n",
            "    Cluster 30: Size:1021 | Avg:0.11 | Min:-0.10 | Max: 0.33\n",
            "    Cluster 38: Size:2316 | Avg:0.10 | Min:-0.06 | Max: 0.33\n",
            "    Cluster 20: Size:4522 | Avg:0.10 | Min:-0.02 | Max: 0.26\n",
            "    Cluster 24: Size:587 | Avg:0.10 | Min:-0.11 | Max: 0.37\n",
            "    Cluster 7: Size:1918 | Avg:0.09 | Min:-0.14 | Max: 0.29\n",
            "    Cluster 47: Size:2987 | Avg:0.09 | Min:-0.04 | Max: 0.27\n",
            "    Cluster 36: Size:1382 | Avg:0.08 | Min:-0.09 | Max: 0.32\n",
            "    Cluster 2: Size:1090 | Avg:0.08 | Min:-0.16 | Max: 0.32\n",
            "    Cluster 27: Size:2161 | Avg:0.08 | Min:-0.13 | Max: 0.31\n",
            "    Cluster 13: Size:1727 | Avg:0.08 | Min:-0.11 | Max: 0.27\n",
            "    Cluster 3: Size:1773 | Avg:0.07 | Min:-0.04 | Max: 0.26\n",
            "    Cluster 22: Size:3201 | Avg:0.07 | Min:-0.04 | Max: 0.25\n",
            "    Cluster 11: Size:2655 | Avg:0.06 | Min:-0.04 | Max: 0.22\n",
            "    Cluster 33: Size:3582 | Avg:0.05 | Min:-0.06 | Max: 0.21\n",
            "    Cluster 45: Size:1285 | Avg:0.05 | Min:-0.20 | Max: 0.32\n",
            "    Cluster 28: Size:398 | Avg:0.04 | Min:-0.20 | Max: 0.29\n",
            "    Cluster 4: Size:2504 | Avg:0.04 | Min:-0.15 | Max: 0.25\n",
            "    Cluster 40: Size:954 | Avg:0.04 | Min:-0.16 | Max: 0.27\n",
            "    Cluster 34: Size:1170 | Avg:0.04 | Min:-0.10 | Max: 0.24\n",
            "    Cluster 43: Size:2158 | Avg:0.04 | Min:-0.16 | Max: 0.21\n",
            "    Cluster 8: Size:1218 | Avg:0.03 | Min:-0.14 | Max: 0.25\n",
            "    Cluster 49: Size:2114 | Avg:0.03 | Min:-0.13 | Max: 0.26\n",
            "    Cluster 9: Size:1486 | Avg:0.03 | Min:-0.12 | Max: 0.22\n",
            "    Cluster 26: Size:2044 | Avg:0.03 | Min:-0.10 | Max: 0.21\n",
            "    Cluster 23: Size:304 | Avg:0.02 | Min:-0.11 | Max: 0.25\n",
            "    Cluster 46: Size:1669 | Avg:0.02 | Min:-0.15 | Max: 0.23\n",
            "    Cluster 16: Size:1283 | Avg:0.01 | Min:-0.16 | Max: 0.23\n",
            "    Cluster 10: Size:1948 | Avg:0.01 | Min:-0.21 | Max: 0.25\n",
            "    Cluster 35: Size:1432 | Avg:0.01 | Min:-0.23 | Max: 0.27\n",
            "    Cluster 6: Size:1038 | Avg:0.01 | Min:-0.20 | Max: 0.26\n",
            "    Cluster 32: Size:2554 | Avg:0.00 | Min:-0.13 | Max: 0.20\n",
            "    Cluster 18: Size:2322 | Avg:0.00 | Min:-0.16 | Max: 0.22\n",
            "    Cluster 48: Size:565 | Avg:0.00 | Min:-0.19 | Max: 0.23\n",
            "    Cluster 5: Size:443 | Avg:-0.00 | Min:-0.19 | Max: 0.23\n",
            "    Cluster 17: Size:950 | Avg:-0.01 | Min:-0.24 | Max: 0.20\n",
            "    Cluster 31: Size:713 | Avg:-0.01 | Min:-0.21 | Max: 0.22\n",
            "    Cluster 29: Size:1422 | Avg:-0.01 | Min:-0.23 | Max: 0.21\n",
            "    Cluster 44: Size:786 | Avg:-0.01 | Min:-0.21 | Max: 0.21\n",
            "    Cluster 25: Size:1629 | Avg:-0.02 | Min:-0.19 | Max: 0.21\n",
            "    Cluster 41: Size:721 | Avg:-0.02 | Min:-0.18 | Max: 0.21\n",
            "    Cluster 0: Size:837 | Avg:-0.02 | Min:-0.24 | Max: 0.24\n",
            "    Cluster 12: Size:1163 | Avg:-0.02 | Min:-0.18 | Max: 0.18\n",
            "    Cluster 14: Size:1230 | Avg:-0.02 | Min:-0.23 | Max: 0.20\n",
            "    Cluster 1: Size:639 | Avg:-0.02 | Min:-0.23 | Max: 0.24\n",
            "    Cluster 21: Size:704 | Avg:-0.03 | Min:-0.24 | Max: 0.23\n",
            "    Cluster 39: Size:733 | Avg:-0.03 | Min:-0.23 | Max: 0.22\n",
            "    Cluster 15: Size:1288 | Avg:-0.03 | Min:-0.23 | Max: 0.20\n",
            "    Cluster 42: Size:658 | Avg:-0.06 | Min:-0.29 | Max: 0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model.wv.vectors\n",
        "kmeans = KMeans(n_clusters=4, n_init='auto').fit(word_vectors)\n",
        "cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
        "print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")"
      ],
      "metadata": {
        "id": "6km4sILQm6do",
        "outputId": "f9cecd91-95b0-4b4d-fb82-791f8661b7fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of elements assigned to each cluster: [94881  8699   935  2404]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce dimensionality to two dimensions using T-SNE"
      ],
      "metadata": {
        "id": "lwXlJAIRpscK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arrays = np.empty((0, 300), dtype='f')\n",
        "word_labels = [word]\n",
        "color_list  = ['red']\n",
        "\n",
        "# adds the vector of the query word\n",
        "arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
        "\n",
        "# gets list of most similar words\n",
        "close_words = model.wv.most_similar([word])\n",
        "\n",
        "# adds the vector for each of the closest words to the array\n",
        "for wrd_score in close_words:\n",
        "  wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "  word_labels.append(wrd_score[0])\n",
        "  color_list.append('blue')\n",
        "  arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "\n",
        "# adds the vector for each of the words from list_names to the array\n",
        "for wrd in list_names:\n",
        "  wrd_vector = model.wv.__getitem__([wrd])\n",
        "  word_labels.append(wrd)\n",
        "  color_list.append('green')\n",
        "  arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "\n",
        "# Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
        "reduc = PCA(n_components=20).fit_transform(arrays)\n",
        "\n",
        "# Finds t-SNE coordinates for 2 dimensions\n",
        "np.set_printoptions(suppress=True)"
      ],
      "metadata": {
        "id": "p1lqJHJTBfcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2, angle=0.2, perplexity=30, learning_rate=200, n_iter=250)\n",
        "tsne_results = tsne.fit_transform(word_vectors)"
      ],
      "metadata": {
        "id": "CKKUYK3yprbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize t-SNE representations of the most common words"
      ],
      "metadata": {
        "id": "uSyXW51bsoOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Display on graph/plot the results based on the clustering performed in k-mins before\n",
        "\n",
        "# Create a scatter plot with different colors for each cluster\n",
        "plt.figure(figsize=(10, 7))\n",
        "for i in range(len(tsne_results)):\n",
        "  plt.scatter(tsne_results[i, 0], tsne_results[i, 1], c=kmeans.labels_[i], alpha=0.5)\n",
        "plt.title('Visualization of Word Clusters using T-SNE')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.legend(cluster_ids)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kf1huGnWoCW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}